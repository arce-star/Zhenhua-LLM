# Zhenhua-LLM (æŒ¯åå¤§æ¨¡å‹)

> **Towards Stylistic Nuance: Fine-tuning Large Language Models for Literary Style Replication of August Changan**

<p align="center">
  <!-- GitHub Stars -->
  <a href="https://github.com/arce-star/Zhenhua-LLM/stargazers" target="_black">
    <img src="https://img.shields.io/github/stars/arce-star/Zhenhua-LLM?style=flat" alt="GitHub Stars"></a>
  <!-- Hugging Face æ¨¡å‹é¡µ -->
  <a href="https://huggingface.co/NQworker/Zhenhua-7B-August-Style" target="_blank">
    <img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-yellow?style=flat" alt="Hugging Face"></a>
  <!-- å¼€æºåè®® -->
  <img src="https://img.shields.io/badge/License-MIT-blue.svg?style=flat" alt="License">
  <!-- Python ç‰ˆæœ¬ -->
  <img src="https://img.shields.io/badge/Python-3.11+-green.svg?style=flat" alt="Python">
</p>

<p align="center">
  <a href="./README.md"><img alt="English" src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_zh.md"><img alt="ç®€ä½“ä¸­æ–‡" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-d9d9d9"></a>
</p>

## ğŸ“– Abstract / æ‘˜è¦

**Zhenhua-LLM** æ˜¯ä¸€ä¸ªä¸“æ³¨äº**æ–‡å­¦é£æ ¼è¿ç§»**ï¼ˆText Style Transferï¼‰ä¸**ç‰¹å®šè§’è‰²äººæ ¼é”šå®š**ï¼ˆCharacter Personality Anchoringï¼‰çš„å¼€æºç ”ç©¶é¡¹ç›®ã€‚æœ¬é¡¹ç›®åŸºäºé€šä¹‰åƒé—® `Qwen2.5-7B-Instruct` åŸºåº§ï¼Œé€šè¿‡æ”¶é›†å¹¶æ·±åº¦å¤„ç†ä¸­å›½è‘—åä½œå®¶å…«æœˆé•¿å®‰çš„â€œæŒ¯åä¸‰éƒ¨æ›²â€ï¼ˆã€Šæœ€å¥½çš„æˆ‘ä»¬ã€‹ã€ã€Šä½ å¥½ï¼Œæ—§æ—¶å…‰ã€‹ã€ã€Šæš—æ‹Â·æ©˜ç”Ÿæ·®å—ã€‹ï¼‰å…¨é‡æ­£æ–‡è¯­æ–™ï¼Œåˆ©ç”¨ **LoRA (Low-Rank Adaptation)** è½»é‡åŒ–å¾®è°ƒæŠ€æœ¯ï¼ŒæˆåŠŸå¤åˆ»äº†å…¶ç»†è…»ã€æ¸…å†·ä¸”å¯Œå«å“²ç†çš„æ–‡å­¦ç¬”è§¦ã€‚

æœ¬é¡¹ç›®ä¸ä»…å®ç°äº†æ–‡å­¦æ„è±¡çš„ç²¾å‡†è¿˜åŸï¼ˆå¦‚â€œçº¢æ¦œâ€ã€â€œç©¿å ‚é£â€ã€â€œä¾§è„¸â€ç­‰ï¼‰ï¼Œè¿˜é’ˆå¯¹ç”Ÿæˆæ¨¡å‹å¸¸è§çš„â€œé€»è¾‘ç¢ç‰‡åŒ–â€å’Œâ€œçŸ­æ–‡æœ¬åè§â€è¿›è¡Œäº†ä¸“é¡¹ä¼˜åŒ–ï¼Œèƒ½å¤Ÿç¨³å®šäº§å‡ºå…·æœ‰é«˜åº¦å™äº‹è¿è´¯æ€§çš„é•¿ç¯‡æ–‡å­¦æ®µè½ã€‚

---

## ğŸ› ï¸ Methodology / æ ¸å¿ƒæŠ€æœ¯è·¯çº¿

### 1. æ•°æ®å·¥ç¨‹ (Data Engineering)
æœ¬é¡¹ç›®æ‘’å¼ƒäº†ä¼ ç»Ÿçš„æ–‡æœ¬å‡åŒ€åˆ‡åˆ†æ³•ï¼Œé‡‡ç”¨äº†**æ®µè½çº§æ»‘åŠ¨çª—å£ (Paragraph-level Sliding Window)** ä¸ **é•¿æ–‡å¼ºåŒ–å¼•å¯¼ (Length-Bias Training)** ç­–ç•¥ï¼š

*   **æ®µè½çº§è¿è´¯æ€§ (Paragraph-level Continuity)**ï¼šé‡‡ç”¨è·¨åº¦ä¸º 2 ä¸ªè‡ªç„¶æ®µçš„æ»‘åŠ¨æ­¥è¿›ï¼ˆStrideï¼‰ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®ä¸­åŒ…å«å®Œæ•´çš„æ–‡å­¦å™äº‹å•å…ƒã€‚ç›¸æ¯”äºç¡¬æ€§çš„ Token åˆ‡åˆ†ï¼Œè¿™ç§æ–¹æ³•èƒ½æ›´å¥½åœ°ä¿ç•™å…«æœˆé•¿å®‰ä½œå“ä¸­ç‰¹æœ‰çš„è½¬åœºé€»è¾‘ä¸å™äº‹èŠ‚å¥ã€‚
*   **é•¿æ–‡è¾“å‡ºå¼ºåŒ– (Long-form Reinforcement)**ï¼šé’ˆå¯¹æ€§æ„é€  **1-2 ä¸ªè‡ªç„¶æ®µè¾“å…¥ -> 5-12 ä¸ªè‡ªç„¶æ®µé«˜è´¨é‡è¾“å‡º** çš„æ˜ å°„å¯¹ã€‚é€šè¿‡å¼ºåˆ¶è®¾ç½®è¾“å‡ºå†…å®¹å¿…é¡»è¶…è¿‡ 400 å­—ï¼Œä»æ•°æ®å±‚çº§è§£å†³äº†æ¨¡å‹ç”Ÿæˆâ€œæŒ¤ç‰™è†â€çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ–‡å­¦åˆ›ä½œçš„ç»†è…»ç¨‹åº¦ã€‚
*   **åŠ¨æ€äººè®¾é”šå®š (Dynamic Persona Anchoring)**ï¼šè„šæœ¬åœ¨æŒ‡ä»¤å±‚çº§ï¼ˆInstructionï¼‰å†…ç½®äº†é€»è¾‘åˆ¤æ–­ï¼Œå½“ä¸Šä¸‹æ–‡æ£€æµ‹åˆ°â€œæ´›æ³â€ã€â€œä½™æ·®â€ç­‰å…³é”®äººç‰©æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ‡æ¢è‡³ä¸“ä¸ºäººè®¾å®šåˆ¶çš„**é”šå®šæŒ‡ä»¤ï¼ˆPersona Promptï¼‰**ï¼Œå®ç°æ–‡é£å¤åˆ»ä¸è§’è‰²ç‰¹è´¨çš„æ·±åº¦è€¦åˆã€‚

### 2. è¶…å‚æ•°è§„èŒƒ (Hyperparameter Paradigm)
è®­ç»ƒåŸºäº `LLaMA-Factory` æ¡†æ¶ï¼Œåœ¨ NVIDIA RTX 3090 (24GB) ç®—åŠ›å¹³å°ä¸Šå®Œæˆï¼Œæ ¸å¿ƒå‚æ•°é…ç½®å¦‚ä¸‹ï¼š

| Parameter | Value | Description |
| :--- | :--- | :--- |
| **Base Model** | Qwen2.5-7B-Instruct | SOTA Chinese open-source LLM |
| **Method** | LoRA | Low-Rank Adaptation (all linear layers) |
| **Rank / Alpha** | 32 / 64 | Balanced capacity for style and knowledge |
| **Learning Rate** | 1e-4 | Cosine decaying scheduler |
| **Cutoff Length** | 2048 | Crucial for modeling long literary passages |
| **Epochs** | 3.0 - 5.0 | Optimized to prevent catastrophic forgetting |

---

## ğŸ“ˆ å®éªŒè¯„ä¼° / Evaluation

### å®šæ€§åˆ†æ (Qualitative Analysis)
å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZhenhua-LLM åœ¨ä»¥ä¸‹ç»´åº¦æ˜¾è‘—ä¼˜äºé€šç”¨æŒ‡ä»¤æ¨¡å‹ï¼š
1.  **é£æ ¼å¿ å®åº¦ (Stylistic Fidelity)**ï¼šèƒ½å¤Ÿè‡ªå¦‚è¿ç”¨å…«æœˆé•¿å®‰æ ‡å¿—æ€§çš„â€œç¬¬äºŒäººç§°å¿ƒç†æå†™â€å’Œâ€œé€šæ„Ÿæ¯”å–»â€ã€‚
2.  **æƒ…æ„Ÿå…±é¸£ (Affective Resonance)**ï¼šæ˜¾è‘—å¢å¼ºäº†å¿ƒç†æå†™å æ¯”ï¼Œèƒ½ç»†è…»å¤„ç†â€œå‘å¾®ä¸éª„å‚²å¹¶å­˜â€çš„å¤æ‚æš—æ‹æƒ…æ„Ÿã€‚
3.  **å™äº‹è¿è´¯æ€§ (Narrative Cohesion)**ï¼šåœ¨é•¿æ–‡ç”ŸæˆæŒ‡ä»¤ä¸‹ï¼Œæ¨¡å‹è¾“å‡ºçš„å¹³å‡ Token é•¿åº¦æå‡äº† 300%ï¼Œä¸”é€»è¾‘ä¸­è½´ç¨³å®šã€‚

> *Case Study: [View Detailed Samples Here](./samples/luozhi.md)*

---

## ğŸš€ Quick Start / å¿«é€Ÿå¼€å§‹

### 1. Installation
```bash
git clone https://github.com/arce-star/Zhenhua-LLM.git
cd Zhenhua-LLM
pip install -r requirements.txt
```

### 2. Model Inference
æœ¬åœ°æ¨ç†å»ºè®®è°ƒç”¨æœ¬ä»“åº“æä¾›çš„ `batch_infer.py` è„šæœ¬ã€‚è¯¥è„šæœ¬å·²å†…ç½®ä¼˜åŒ–çš„é‡‡æ ·å‚æ•°ï¼š
- **Repetition Penalty**: 1.05 (æœ‰æ•ˆé˜²æ­¢é•¿æ–‡ç”Ÿæˆçš„å¾ªç¯å†—ä½™)
- **Temperature**: 0.85 (ä¿æŒæ–‡å­¦åˆ›ä½œæ‰€éœ€çš„éšæœºæ€§çµæ„Ÿ)
- **Top-p**: 0.9

```bash
python batch_infer.py --model_path /path/to/your/merged_model
```

---

## ğŸ“… Roadmap / å¼€å‘è·¯çº¿
- [x] **v1.0**: é£æ ¼åŒ– SFT å®Œæˆï¼Œåˆæ­¥å¤åˆ»â€œæŒ¯åå‘³â€è¯­æ„Ÿã€‚
- [x] **v2.0**: å¼•å…¥æ»‘åŠ¨çª—å£è®­ç»ƒï¼Œè§£å†³é•¿æ–‡æœ¬ç”Ÿæˆé€»è¾‘å´©åã€‚
- [ ] **v3.0 (In Progress)**: è§’è‰²æ‰®æ¼”ä¸“é¡¹å¢å¼ºï¼ˆRP-Enhancedï¼‰ï¼Œå®ç°å¤šäººç‰©äººæ ¼ä¸æ··æ·†ã€‚
- [ ] **v4.0**: å¼•å…¥ RAG (Retrieval-Augmented Generation)ï¼Œå®ç°å¯¹â€œæŒ¯åå®‡å®™â€ä¸‡ä½™ä¸ªç»†èŠ‚çš„ç²¾å‡†çŸ¥è¯†æ£€ç´¢ã€‚

---

## âš ï¸ Disclaimer / å…è´£å£°æ˜
1.  **ç‰ˆæƒå£°æ˜**ï¼šæœ¬é¡¹ç›®æ‰€ä½¿ç”¨çš„è®­ç»ƒè¯­æ–™ç‰ˆæƒå½’åŸä½œè€…å…«æœˆé•¿å®‰åŠå…¶æ‰€å±å‡ºç‰ˆæœºæ„æ‰€æœ‰ã€‚
2.  **ç”¨é€”é™åˆ¶**ï¼šæœ¬é¡¹ç›®ä»…ä¾›è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å­¦æœ¯ç ”ç©¶ä¸æŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œ**ä¸¥ç¦ä»»ä½•å½¢å¼çš„å•†ä¸šç”¨é€”ã€å¤§è§„æ¨¡ä¼ æ’­æˆ–å‡ºç‰ˆè¡Œä¸º**ã€‚
3.  **é£é™©æç¤º**ï¼šæ¨¡å‹è¾“å‡ºåŸºäºæ¦‚ç‡ç”Ÿæˆï¼Œå¯èƒ½åŒ…å«ä¸å¯æ§çš„å¹»è§‰ï¼ˆHallucinationï¼‰ï¼Œä¸ä»£è¡¨é¡¹ç›®ä½œè€…ç«‹åœºï¼Œäº¦ä¸ä»£è¡¨åŸä½œè€…çš„çœŸå®åˆ›ä½œæ„å›¾ã€‚

---

## ğŸ¤ Acknowledgement / è‡´è°¢
- æ„Ÿè°¢ **å…«æœˆé•¿å®‰** åˆ›ä½œäº†æ¸©æš–æ— æ•°äººçš„æŒ¯åæ•…äº‹ã€‚
- æ„Ÿè°¢ **Alibaba Qwen Team** æä¾›å¼ºå¤§çš„åŸºåº§æ¨¡å‹æ”¯æŒã€‚
- æ„Ÿè°¢ **LLaMA-Factory** æä¾›çš„å¾®è°ƒæ¡†æ¶æ”¯æŒã€‚
